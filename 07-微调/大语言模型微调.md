

#### SFT

大语言模型的有监督微调（Supervised Fine-Tuning，简称SFT）是将预训练模型在特定任务的数据集上进一步训练的过程，目的是使模型更好地适应该任务的特点和需求。在实践中，有多种方法可以用于大语言模型的有监督微调，以下是一些主流的方法：

全参数微调（Full Fine-Tuning） 25

这是最传统的微调方法，即在目标任务的数据集上对预训练模型的所有参数进行微调。这种方法可以使模型充分适应目标任务，但计算成本较高，尤其是对于参数量巨大的模型。
增量式微调（Addition-based Fine-Tuning） 347

这种方法通过在原始模型中添加额外的可训练模块或参数来进行微调，而不是调整原有参数。常见的增量式方法包括Adapter-Tuning、Prefix-Tuning和Prompt-Tuning等。这些方法通过仅微调一小部分参数来降低计算和存储成本，同时保持与全参数微调相当的性能。
指定式微调（Specification-based Fine-Tuning） 347

指定式方法选择原始模型中的特定参数集进行微调，而其他参数保持不变。这种方法不会引入新参数，也不改变模型结构，例如BitFit方法通过优化模型内部的偏置项来达到微调目的。
重参数化微调（Reparameterization-based Fine-Tuning） 347

重参数化方法通过转换模型的参数表示来实现参数高效的微调。例如，LoRA（Low-Rank Adaptation）方法通过在Transformer的每一层注入低秩矩阵来实现微调，大大减少了可训练参数的数量，同时保持了良好的性能34。
冻结微调（Freeze Fine-Tuning） 47

在冻结微调中，大部分模型参数被冻结，只有少数层或特定参数被允许更新。例如，可以只微调Transformer模型的最后几层的全连接层参数，而保持其他层参数不变。
P-tuning v2 微调方法 4

P-tuning v2是一种改进的微调方法，它结合了prefix-tuning和P-tuning v1的优点，在模型的每一层都加入了可微调的参数，同时采用了多任务学习优化，适用于各种规模的语言模型。
这些方法各有优势和适用场景，研究者和开发者可以根据具体任务的需求、可用资源和模型的特点来选择最合适的微调策略。随着大模型技术的不断进步，未来可能会出现更多高效且创新的微调方法。



#### LoRA

LoRA方法的工作原理可以概括为以下几个步骤1：
参数分解：LoRA首先将模型的参数矩阵分解为固定部分和可训练的低秩部分。固定部分保留了原始模型的知识，而低秩部分则用于捕捉任务特定的信息。

微调：在训练过程中，只更新低秩部分的参数。这大大减少了需要训练的参数数量，从而降低了计算成本，并减少了过拟合的风险。

前向传播：在前向传播过程中，将固定部分和低秩部分组合起来，生成模型的输出。

LoRA方法的优势包括1：
高效性：通过仅更新低秩部分，LoRA显著减少了计算资源和训练时间的需求。

灵活性：LoRA可以在保持原始模型通用性的基础上，快速适应不同任务。

适用性：LoRA特别适合在大规模数据集上进行微调，因为它可以在有限的计算资源下实现高效的模型优化。

在实践中，应用LoRA进行微调需要一些技巧和经验1：
选择合适的任务：LoRA适用于那些需要在大规模数据集上进行微调的任务。对于小型数据集或简单任务，传统的微调方法可能更加合适。

调整学习率：由于LoRA只更新低秩部分，学习率的设置变得尤为重要。建议使用较小的学习率，以避免过度拟合。

监控验证集性能：在微调过程中，密切关注验证集的性能是必要的。如果验证集性能开始下降，可以考虑提前停止训练，以防止过拟合。

LoRA在GPT系列模型中的应用取得了显著成效2：
性能提升：通过对GPT-3等大型语言模型进行LoRA微调，研究人员能够在保持模型通用性的基础上，显著提升模型在特定领域（如问答、文本生成等）的性能。

广泛前景：LoRA在其他大型语言模型中也具有广泛的应用前景，预示着它将在更多领域发挥重要作用，推动人工智能技术的不断进步。



#### LoRA的改进

LoRA（Low-Rank Adaptation）作为一种高效的大模型微调方法，已经被广泛应用于各种场景。然而，研究者们也在不断探索对LoRA的改进方法，以解决其在某些情况下的局限性，并进一步提升微调效率和模型性能。以下是一些对LoRA的改进方法：

LoRA+ 2

LoRA+提出了为LoRA的adapter矩阵A和B设置不同的学习率，其中B的学习率是A的学习率的λ倍（λ > 1）。这种方法可以有效提高特征学习效率，特别是在大宽度网络中，通过不同的学习率更新可以更好地捕捉任务相关的特征。LoRA+在保持与LoRA相同计算成本的同时，能够提高性能和微调速度。
PLoRA 2

PLoRA（Progressive Low-Rank Adaptation）的核心思想是通过多次累积低秩更新矩阵来实现更高的更新秩。PLoRA包含多个训练阶段，在每个阶段结束时，将训练得到的低秩矩阵乘积合并到主干参数矩阵中，从而在多个阶段后获得高秩的最终更新。这种方法理论上可以接近全参数微调的效果，而不引入额外的内存开销。
QLoRA 3

QLoRA（Quantized Low-Rank Adaptation）是一种在微调过程中进一步减少内存占用的技术。它通过在反向传播过程中将预训练的权重量化为低位宽度（如4-bit），并使用分页优化器来处理内存峰值。QLoRA可以节省GPU内存，虽然训练时间可能会略有增加，但模型性能几乎不受影响。
ReLoRa 10

ReLoRa是对LoRA的另一种改进，旨在解决有效训练高性能变换器模型方面的困难。ReLoRa通过引入新的方法来优化训练过程，从而提高模型的性能。
VeRA 9

VeRA（Vectorized Low-Rank Adaptation）可能是对LoRA进行向量化优化的尝试，以提高微调过程中的计算效率和模型性能。
LoRA-fa 9

LoRA-fa可能是对LoRA方法的另一种改进，尽管具体细节未在搜索结果中提及，但这种改进可能涉及对LoRA方法的适应性或灵活性方面的优化。
LoRA-drop 9

LoRA-drop可能是一种结合了dropout技术的LoRA改进方法，旨在减少过拟合并提高模型的泛化能力。
AdaLoRA 9

AdaLoRA可能是对LoRA方法的自适应改进，通过调整LoRA层的参数以更好地适应特定的任务或数据集。
这些改进方法展示了LoRA技术的多样性和适应性，同时也反映了研究者们在不断探索如何更有效地利用大型预训练模型以适应各种任务和领域。随着人工智能技术的不断发展，我们可以期待未来会有更多创新的LoRA变体出现。



#### 微调的注意事项

##### 大语言模型base版和chat版的区别是什么？

回答：
base版本更适合文本补全、摘要、翻译和内容生成等任务，而chat版本适合多轮对话的任务。而base版有更强的泛化能力，chat版本是在base版本的基础上进行SFT和RLHF，有更好的对话能力和自然语言理解能力。实际业务环境中，需要根据业务和自身拥有的数据来进行选择。

##### 在SFT的时候是在Base模型上训练还是在Chat模型上训练？

如果只有5k数据，可以在Chat模型上进行微调；如果有10w数据，可以在Base模型上进行微调。因为不知道Chat模型在SFT时具体的数据质量如何

openai的回答
如果预测任务主要与对话系统或需要理解并生成自然语言对话相关，例如聊天机器人、虚拟助手或客户支持场景，那么选择经过特别优化用于处理对话的Chat版本可能更合适。Chat版本通常对上下文理解和维持对话连贯性进行了优化，从而能够提供更自然、流畅且符合对话习惯的回复。
相反，如果您的预测任务主要是基于文本的非对话型任务，如文本分类、实体识别、情感分析或其他类型的文本分析任务，Base版本可能是更好的选择。Base版本通常在更广泛的文本数据上训练，具备了处理多种自然语言处理任务的基础能力。

因此，总结一下关键考虑点：
任务类型：对话型任务选择Chat版本，非对话型任务选择Base版本。
数据类型：如果您的数据集主要是对话形式，选择Chat版本可能更有优势；如果是多种类型的文本，则Base版本更为适宜。
模型性能：考虑任务需求和性能指标选择最适合任务特点的版本。

##### 微调时的模板选择

大模型的微调、对话均需要选择一个合适的对话模版（prompt template）。需要使用对话模版将训练数据和对话文本进行“模版化”，然后输入模型。

> 各种推理引擎也都会用到对话模板，每个框架定义对话模板的方式都不尽相同，但最终“模板化”后的数据都是相同的。
>
> 请确保在训练、对话和自定义应用场景中，始终保持对话模板的一致，否则可能会出现不符合预期的结果。

一般的微调框架都会内置模板可供选择，实际微调时选择合适的即可。选择准确的对话模版是训练、应用模型的关键。关于如何选择对话模版，建议：

* 微调 chat 模型:
  使用模型所对应的对话模版，如 internlm2-chat 使用 internlm2_chat、Qwen-Chat 使用 qwen_chat。

* 全量微调 base 模型:
  任选对话模版，优先使用 chat 版模型所对应的对话模版 。

* LoRA 微调 base 模型:
  使用默认对话模版 default。这是由于 LoRA / QLoRA 微调默认会关闭 embed_tokens 和 lm_head 的训练，此时如果引入未学习过的特殊 token（如对话模版中的 <|im_start|>），则会影响模型的训练。

> 可以通过引入 embedding层和 lm_head 的训练（会增大显存需求），进而支持任选对话模版。如Xtuner支持这种操作。
>
> 大多数的 base 模型所使用的 tokenizer 中不包含 chat 模型对话模板中所使用的特殊 token 编码。因此如果要微调 base 模型并配合使用 chat 版对话模版，需确保全流程使用 chat 版模型的 tokenizer。

参考：

https://xtuner.readthedocs.io/zh-cn/latest/preparation/prompt_template.html

https://blog.csdn.net/mingzai624/article/details/135952802

##### ZeRO3与LoRA无法同时使用

https://github.com/QwenLM/Qwen/issues/1104



##### QLoRA

在常规推理阶段直接量化肯定会导致模型有性能损失。但是QLora是将模型量化后再微调，作者在论文中也做了些对比实验，发现QLora在>7B模型的效果能达到甚至超过常规Lora微调的效果。

Qlora有如此效果的原因有三个：

1）刚刚咱们说的Qlora是先量化再微调，效果肯定比微调后再量化好

2）Qlora不仅有量化，另外作者发现lora adapter的数量越多微调效果也就越好，因此相比于常规Lora只把adapter应用在query和value上，Qlora在所有的线性层上都添加了adapter，数量上要比常规lora多很多

3）NF4要比FP4的精度损失更少

https://zhuanlan.zhihu.com/p/648239462

https://zhuanlan.zhihu.com/p/681519805









Lora：(https://github.com/Lightning-AI/lit-llama/blob/main/lit_llama/lora.py)

QLora

Adapter

BitFit

P-tuning



库：PEFT

LLaMA-Factory

unsloth

xtuner：https://blog.csdn.net/qq_73668945/article/details/135798748

https://blog.csdn.net/qq_73668945/article/details/135563747#t16



torch官方教程

https://pytorch.org/blog/finetune-llms/



llama解读及微调：

https://blog.csdn.net/v_JULY_v/article/details/129709105















