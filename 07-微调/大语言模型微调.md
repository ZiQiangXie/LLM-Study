

#### SFT

大语言模型的有监督微调（Supervised Fine-Tuning，简称SFT）是将预训练模型在特定任务的数据集上进一步训练的过程，目的是使模型更好地适应该任务的特点和需求。在实践中，有多种方法可以用于大语言模型的有监督微调，以下是一些主流的方法：

全参数微调（Full Fine-Tuning） 25

这是最传统的微调方法，即在目标任务的数据集上对预训练模型的所有参数进行微调。这种方法可以使模型充分适应目标任务，但计算成本较高，尤其是对于参数量巨大的模型。
增量式微调（Addition-based Fine-Tuning） 347

这种方法通过在原始模型中添加额外的可训练模块或参数来进行微调，而不是调整原有参数。常见的增量式方法包括Adapter-Tuning、Prefix-Tuning和Prompt-Tuning等。这些方法通过仅微调一小部分参数来降低计算和存储成本，同时保持与全参数微调相当的性能。
指定式微调（Specification-based Fine-Tuning） 347

指定式方法选择原始模型中的特定参数集进行微调，而其他参数保持不变。这种方法不会引入新参数，也不改变模型结构，例如BitFit方法通过优化模型内部的偏置项来达到微调目的。
重参数化微调（Reparameterization-based Fine-Tuning） 347

重参数化方法通过转换模型的参数表示来实现参数高效的微调。例如，LoRA（Low-Rank Adaptation）方法通过在Transformer的每一层注入低秩矩阵来实现微调，大大减少了可训练参数的数量，同时保持了良好的性能34。
冻结微调（Freeze Fine-Tuning） 47

在冻结微调中，大部分模型参数被冻结，只有少数层或特定参数被允许更新。例如，可以只微调Transformer模型的最后几层的全连接层参数，而保持其他层参数不变。
P-tuning v2 微调方法 4

P-tuning v2是一种改进的微调方法，它结合了prefix-tuning和P-tuning v1的优点，在模型的每一层都加入了可微调的参数，同时采用了多任务学习优化，适用于各种规模的语言模型。
这些方法各有优势和适用场景，研究者和开发者可以根据具体任务的需求、可用资源和模型的特点来选择最合适的微调策略。随着大模型技术的不断进步，未来可能会出现更多高效且创新的微调方法。



#### LoRA

LoRA方法的工作原理可以概括为以下几个步骤1：
参数分解：LoRA首先将模型的参数矩阵分解为固定部分和可训练的低秩部分。固定部分保留了原始模型的知识，而低秩部分则用于捕捉任务特定的信息。

微调：在训练过程中，只更新低秩部分的参数。这大大减少了需要训练的参数数量，从而降低了计算成本，并减少了过拟合的风险。

前向传播：在前向传播过程中，将固定部分和低秩部分组合起来，生成模型的输出。

LoRA方法的优势包括1：
高效性：通过仅更新低秩部分，LoRA显著减少了计算资源和训练时间的需求。

灵活性：LoRA可以在保持原始模型通用性的基础上，快速适应不同任务。

适用性：LoRA特别适合在大规模数据集上进行微调，因为它可以在有限的计算资源下实现高效的模型优化。

在实践中，应用LoRA进行微调需要一些技巧和经验1：
选择合适的任务：LoRA适用于那些需要在大规模数据集上进行微调的任务。对于小型数据集或简单任务，传统的微调方法可能更加合适。

调整学习率：由于LoRA只更新低秩部分，学习率的设置变得尤为重要。建议使用较小的学习率，以避免过度拟合。

监控验证集性能：在微调过程中，密切关注验证集的性能是必要的。如果验证集性能开始下降，可以考虑提前停止训练，以防止过拟合。

LoRA在GPT系列模型中的应用取得了显著成效2：
性能提升：通过对GPT-3等大型语言模型进行LoRA微调，研究人员能够在保持模型通用性的基础上，显著提升模型在特定领域（如问答、文本生成等）的性能。

广泛前景：LoRA在其他大型语言模型中也具有广泛的应用前景，预示着它将在更多领域发挥重要作用，推动人工智能技术的不断进步。



#### LoRA的改进

LoRA（Low-Rank Adaptation）作为一种高效的大模型微调方法，已经被广泛应用于各种场景。然而，研究者们也在不断探索对LoRA的改进方法，以解决其在某些情况下的局限性，并进一步提升微调效率和模型性能。以下是一些对LoRA的改进方法：

LoRA+ 2

LoRA+提出了为LoRA的adapter矩阵A和B设置不同的学习率，其中B的学习率是A的学习率的λ倍（λ > 1）。这种方法可以有效提高特征学习效率，特别是在大宽度网络中，通过不同的学习率更新可以更好地捕捉任务相关的特征。LoRA+在保持与LoRA相同计算成本的同时，能够提高性能和微调速度。
PLoRA 2

PLoRA（Progressive Low-Rank Adaptation）的核心思想是通过多次累积低秩更新矩阵来实现更高的更新秩。PLoRA包含多个训练阶段，在每个阶段结束时，将训练得到的低秩矩阵乘积合并到主干参数矩阵中，从而在多个阶段后获得高秩的最终更新。这种方法理论上可以接近全参数微调的效果，而不引入额外的内存开销。
QLoRA 3

QLoRA（Quantized Low-Rank Adaptation）是一种在微调过程中进一步减少内存占用的技术。它通过在反向传播过程中将预训练的权重量化为低位宽度（如4-bit），并使用分页优化器来处理内存峰值。QLoRA可以节省GPU内存，虽然训练时间可能会略有增加，但模型性能几乎不受影响。
ReLoRa 10

ReLoRa是对LoRA的另一种改进，旨在解决有效训练高性能变换器模型方面的困难。ReLoRa通过引入新的方法来优化训练过程，从而提高模型的性能。
VeRA 9

VeRA（Vectorized Low-Rank Adaptation）可能是对LoRA进行向量化优化的尝试，以提高微调过程中的计算效率和模型性能。
LoRA-fa 9

LoRA-fa可能是对LoRA方法的另一种改进，尽管具体细节未在搜索结果中提及，但这种改进可能涉及对LoRA方法的适应性或灵活性方面的优化。
LoRA-drop 9

LoRA-drop可能是一种结合了dropout技术的LoRA改进方法，旨在减少过拟合并提高模型的泛化能力。
AdaLoRA 9

AdaLoRA可能是对LoRA方法的自适应改进，通过调整LoRA层的参数以更好地适应特定的任务或数据集。
这些改进方法展示了LoRA技术的多样性和适应性，同时也反映了研究者们在不断探索如何更有效地利用大型预训练模型以适应各种任务和领域。随着人工智能技术的不断发展，我们可以期待未来会有更多创新的LoRA变体出现。













Lora：(https://github.com/Lightning-AI/lit-llama/blob/main/lit_llama/lora.py)

QLora

Adapter

BitFit

P-tuning



库：PEFT

LLaMA-Factory

unsloth

xtuner：https://blog.csdn.net/qq_73668945/article/details/135798748

https://blog.csdn.net/qq_73668945/article/details/135563747#t16



torch官方教程

https://pytorch.org/blog/finetune-llms/



llama解读及微调：

https://blog.csdn.net/v_JULY_v/article/details/129709105















