





大语言模型主要以Decoder为主，推理时主要是自回归（auto regressive）的方式。







技术：

https://developer.baidu.com/article/detail.html?id=919629

https://blog.csdn.net/qq_36426650/article/details/130764843



KVCache

https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/

https://blog.csdn.net/LF_AI/article/details/130838524

MQA、GQA



投机采样

多头美杜莎

https://www.birentech.com/Research_nstitute_details/24.html



分布式推理

PagedAttention 

kernel fusion

低bit量化

fp16

IO优化

计算优化

flash attention等，通用方法，即可训练又可推理

GPTQ、AutoGPTQ

Dynamic SplitFuse，源自DeepSpeed-FastGen

batching：in-flight batching/continuous batching、 naive/static batching

https://blog.csdn.net/qq_27590277/article/details/135710435



知名推理加速库：

vLLM：vLLM是一个开源的大模型推理加速框架,通过PagedAttention高效地管理attention中缓存的张量,实现了比HuggingFace Transformers高14-24倍的吞吐量。 PagedAttention 是 vLLM 的核心

Tensorrt-LLM：MQA、GQA

GPT-fast

llama.cpp

Faster Transformer

lmdeploy

AirLLM：分层推理，4GB显存推理70B模型，无需量化压缩；

PowerInfer：结合大模型的独特特征，通过 CPU 与 GPU 间的混合计算，PowerInfer 能够在显存有限的个人电脑上实现快速推理。

含有推理库汇总：https://github.com/HqWu-HITCS/Awesome-Chinese-LLM

### 采样

**Greedy Search（Maximization）**

Greedy search方法的思想较为简单，就是直接选择概率分布中概率最大的token（（或字符））作为当前解码出来的词（或字符）。 但是，该方法的问题在于，如果我们总是选择概率最大的词，将会生成很多重复的句子( get stuck in loops )，例如“I don’t know. I don’t know. I don’t know. I don’t know.”

**Beam Search**

该方法是对greedy search的扩展版本，返回一系列最有可能的输出序列。和greedy search选择可能性最大的构成序列不同，beam search在t步时，生成$t + 1$步的所有可能组成，并从中选择k个概率最大的组合，其中k为指定的搜索参数。
在开始的位置不用随机选择，而是选择K个最可能在开始位置的词语作为序列的第一个词。
当K取1时，即为Greedy Search，而在大多数机器翻译的任务中 K一般取值5-10。当K较大时，往往会带来较好的结果，因为保留更多的选择性，更可能带来最佳的组合，相应的，也会增加计算成本和解码速度。
举例说明上述表述。首先，定义一个函数，该函数在给定一个序列（假设长度为N, 词汇表长度为V）的概率分布(矩阵，N x V)，以及搜索参数K时，得到解码结果。在每一个step，每一个候选子序列( candidate sequence)扩展所有可能的下一个子token，然后按照score进行排序，并选择score最大的K个子序列，作为当前step的解码结果。重复上述过程，直到迭代结束。
一般来说，概率值时较小的数值，经过一些列连乘后，会更小，为防止下溢（underflowing the floating point numbers),将其计算转换为取其对数，然后相加的过程。

**Temperature Sampling**

 Temperature sampling的想法源于热力统计学的概念，温度高往往意味着更容易是低能量状态。在概率模型中， logits 代表能量值，将其送入softmax函数钱，除以temperature值，得到最终的采样概率分布。

**Top-K Sampling**

在Top-K 采样中, 依旧是从概率分布中，依据概率最大选择k个单词中，不同的点在于，该方法会对这K个词的概率重新再次进行分布(redistributed)，然后依据新的概率分布重新取下一个token。GPT2模型就是用的这种采样方法，使其在故事生成（story generation）方面较为成熟。

https://arxiv.org/abs/1805.04833

**Top-P Sampling（Nucleus Sampling）**

与top k对低概率词汇直接丢弃的处理方法不同，top p采用的是累计概率的方式。即从累计概率超过某一个阈值p的词汇中进行采样。换言之，根据参数p的大小调节(0<=p<=1), Top-P Sampling增大了出现概率较小的词汇的生成的概率。

【参考】https://www.cnblogs.com/miners/p/14950681.html



### Kernel优化和算子融合

在NVIDIA GPU环境上，我们通过CUDA Kernel来执行大部分运算，矩阵乘法（GEMM），激活函数，softmax等，一般老说每个操作都对应一次kernel调用。但是每次kernel调用都有一些额外开销，比如gpu和cpu之间的通信，内存拷贝等，因此我们可以将整个Attention的计算放进同一个kernel实现，省略这些开销，在kernel中也可以实现一些Attention专用的优化。比如Facebook的xformers库就为各种Attention变种提供了高效的CUDA实现。主流的推理库也基本都自带了高效的Kernel实现。

除了手写的Kernel外，模型编译器也可以提供类似的优化机制，编译器将模型结构转换为其中间格式然后进行一系列的优化，比如我们提到的算子融合。虽然在CUDA这种主流平台上编译器的优化效果不一定比得上专用的Kernel实现，但是多平台的通用性是编译器的一大优势，比如TVM团队的MLC-LLM和微软主导的ONNX常被用来在手机等边缘设备上运行大模型。

【参考】https://blog.csdn.net/javastart/article/details/136291419