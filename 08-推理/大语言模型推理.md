





大语言模型主要以Decoder为主，推理时主要是自回归（auto regressive）的方式。







技术：

https://developer.baidu.com/article/detail.html?id=919629

https://blog.csdn.net/qq_36426650/article/details/130764843



KVCache

https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/

https://blog.csdn.net/LF_AI/article/details/130838524

MQA、GQA



投机采样

多头美杜莎

https://www.birentech.com/Research_nstitute_details/24.html



分布式推理

PagedAttention 

kernel fusion

低bit量化: GPTQ、AutoGPTQ、AWQ、auto AWQ、SmoothQuant、LLM.int8()、K-Quants、imatrix增强的K-Quants、i-quants

fp16

IO优化

计算优化

flash attention等，通用方法，即可训练又可推理

Dynamic SplitFuse，源自DeepSpeed-FastGen

batching：in-flight batching/continuous batching、 naive/static batching

https://blog.csdn.net/qq_27590277/article/details/135710435



知名推理加速库：

transformers: # Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.

vLLM：vLLM是一个开源的大模型推理加速框架,通过PagedAttention高效地管理attention中缓存的张量,实现了比HuggingFace Transformers高14-24倍的吞吐量。 PagedAttention 是 vLLM 的核心

Tensorrt-LLM：MQA、GQA

GPT-fast

llama.cpp

Faster Transformer

lmdeploy

rtp-llm：阿里推出，融合多种库的技巧；

AirLLM：分层推理，4GB显存推理70B模型，无需量化压缩；

PowerInfer：结合大模型的独特特征，通过 CPU 与 GPU 间的混合计算，PowerInfer 能够在显存有限的个人电脑上实现快速推理。

含有推理库汇总：https://github.com/HqWu-HITCS/Awesome-Chinese-LLM



|              | py       | cuda           | torch             | transformers      | triton          |
| ------------ | -------- | -------------- | ----------------- | ----------------- | --------------- |
| transformers |          |                |                   |                   |                 |
| lmdeploy     | 3.8-3.11 | 11.8, 12.+     | <=2.1.2,>=2.0.0   | >=4.33.0,<=4.38.1 | >=2.1.0,<=2.2.0 |
| TRT-llm      | 3.10     | 12.1(eg)       | >=2.1.0a,<=2.2.0a | ==4.38.2          |                 |
| vLLM         | 3.8–3.11 | 11.8, **12.1** | =2.1.2            | >= 4.38.0         | >= 2.1.0        |
| AirLLM       |          |                |                   |                   |                 |
| rtp-llm      | 3.10     | 11.8， 12.1    | =2.1.0            | =4.33.1           |                 |
| llama.cpp    |          |                |                   |                   |                 |
| Xinference   |          |                |                   |                   |                 |
| PowerInfer   |          |                |                   |                   |                 |



### 采样

**Greedy Search（Maximization）**

Greedy search方法的思想较为简单，就是直接选择概率分布中概率最大的token（（或字符））作为当前解码出来的词（或字符）。 但是，该方法的问题在于，如果我们总是选择概率最大的词，将会生成很多重复的句子( get stuck in loops )，例如“I don’t know. I don’t know. I don’t know. I don’t know.”

**Beam Search**

该方法是对greedy search的扩展版本，返回一系列最有可能的输出序列。和greedy search选择可能性最大的构成序列不同，beam search在t步时，生成$t + 1$步的所有可能组成，并从中选择k个概率最大的组合，其中k为指定的搜索参数。
在开始的位置不用随机选择，而是选择K个最可能在开始位置的词语作为序列的第一个词。
当K取1时，即为Greedy Search，而在大多数机器翻译的任务中 K一般取值5-10。当K较大时，往往会带来较好的结果，因为保留更多的选择性，更可能带来最佳的组合，相应的，也会增加计算成本和解码速度。
举例说明上述表述。首先，定义一个函数，该函数在给定一个序列（假设长度为N, 词汇表长度为V）的概率分布(矩阵，N x V)，以及搜索参数K时，得到解码结果。在每一个step，每一个候选子序列( candidate sequence)扩展所有可能的下一个子token，然后按照score进行排序，并选择score最大的K个子序列，作为当前step的解码结果。重复上述过程，直到迭代结束。
一般来说，概率值时较小的数值，经过一些列连乘后，会更小，为防止下溢（underflowing the floating point numbers),将其计算转换为取其对数，然后相加的过程。

**Temperature Sampling**

 Temperature sampling的想法源于热力统计学的概念，温度高往往意味着更容易是低能量状态。在概率模型中， logits 代表能量值，将其送入softmax函数钱，除以temperature值，得到最终的采样概率分布。

**Top-K Sampling**

在Top-K 采样中, 依旧是从概率分布中，依据概率最大选择k个单词中，不同的点在于，该方法会对这K个词的概率重新再次进行分布(redistributed)，然后依据新的概率分布重新取下一个token。GPT2模型就是用的这种采样方法，使其在故事生成（story generation）方面较为成熟。

https://arxiv.org/abs/1805.04833

**Top-P Sampling（Nucleus Sampling）**

与top k对低概率词汇直接丢弃的处理方法不同，top p采用的是累计概率的方式。即从累计概率超过某一个阈值p的词汇中进行采样。换言之，根据参数p的大小调节(0<=p<=1), Top-P Sampling增大了出现概率较小的词汇的生成的概率。

【参考】https://www.cnblogs.com/miners/p/14950681.html



### Kernel优化和算子融合

在NVIDIA GPU环境上，我们通过CUDA Kernel来执行大部分运算，矩阵乘法（GEMM），激活函数，softmax等，一般老说每个操作都对应一次kernel调用。但是每次kernel调用都有一些额外开销，比如gpu和cpu之间的通信，内存拷贝等，因此我们可以将整个Attention的计算放进同一个kernel实现，省略这些开销，在kernel中也可以实现一些Attention专用的优化。比如Facebook的xformers库就为各种Attention变种提供了高效的CUDA实现。主流的推理库也基本都自带了高效的Kernel实现。

除了手写的Kernel外，模型编译器也可以提供类似的优化机制，编译器将模型结构转换为其中间格式然后进行一系列的优化，比如我们提到的算子融合。虽然在CUDA这种主流平台上编译器的优化效果不一定比得上专用的Kernel实现，但是多平台的通用性是编译器的一大优势，比如TVM团队的MLC-LLM和微软主导的ONNX常被用来在手机等边缘设备上运行大模型。

【参考】https://blog.csdn.net/javastart/article/details/136291419



### **lmdeploy**

https://blog.csdn.net/qq_73668945/article/details/135712890

https://blog.csdn.net/weixin_42475060/article/details/135386145



### vllm

pagedattention

https://cloud.tencent.com/developer/article/2332926

https://blog.csdn.net/muyao987/article/details/131690801

https://blog.csdn.net/taoqick/article/details/131382360



### flashattention

GPU基础：

https://blog.csdn.net/JackZhang_123/article/details/78147197

https://blog.csdn.net/Kevyn7/article/details/115899559

flashattentionv1

http://fancyerii.github.io/2023/10/23/flashattention/

https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh

https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247519461&amp;idx=1&amp;sn=292e647dd10734e3ae8bfc00b7f0e645&amp;chksm=9f833673a8f4bf653718401443fb09e71f349dafc59df43260f13b284465e927eaf5a504bc0f&amp;scene=21#wechat_redirect

https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247525362&idx=1&sn=28474ea7317931d291b87bfb3e7f5995&chksm=eb537761dc24fe773248f7b85d9b56047317b7f2a70fda94f295e3bf146453739ab359071995&mpshare=1&scene=1&srcid=0222aM5LWOMjkUtqCpBqIqjJ&sharer_shareinfo=880039d3cabf84db9028433c53e7a887&sharer_shareinfo_first=880039d3cabf84db9028433c53e7a887&exportkey=n_ChQIAhIQik8WMs5vLuHdIsqt0PXoQhKYAgIE97dBBAEAAAAAAH3iDr933q0AAAAOpnltbLcz9gKNyK89dVj0OrK%2BEIgrlcA0BkannItOdRQDxRGNXLbc13nWBcWPeKnD2B8rMMAjS%2BShWyU6gCnYCqvkk3pOWcPby2PeE3ibNHpZfGlQFucPUR%2F6F3QrPpm0S1%2F%2F%2BRb7WD3xkL8mfRgWPgupiVno2oO6ewDJu%2B7kOg2LBT4%2FqlQaqPyOWYFprMJZe7NH0RMQwLVMv%2F37yv4Nu3FiIIZ8bTMxZUzMotV95GpbJ6qsiWVY3G2cHlXj0IN%2FRQp%2F5%2FlZU%2FPFv4P%2BsRxVbc4oceBKRxSsw5fQ1XQ8YSs1jUqG7AKgluXH3lpl6jAXgGbPZZqv7q0znfLX%2Fs8ZSs4%3D&acctmode=0&pass_ticket=4VTpNvG1thl3WZY%2BqZCQvaqG6EaFUtwZR3ONy%2FiZlo%2FKMff5c9GM83brEIpNrSnf56vW%2Bdic3LfK00Sl0ZVBNg%3D%3D&wx_header=0#rd

flashattention2

https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&mid=2247519477&idx=1&sn=ee4fe0a47c7f517a9dda9976cc7075f7&chksm=9f833663a8f4bf7529c5d0b63f2ccabdf69ef2189a6e88b3ecadf7ecba3f79f66db4241eb6b8&scene=21#wechat_redirect

flashDecoding

https://pytorch.org/blog/flash-decoding/



量化



【参考】https://cloud.tencent.com/developer/article/2361027