









**LLama：**

LLaMa与普通的Transformer架构不同的地方，包括采用了前置了层归一化（Pre-normalization）并使用RMSNorm 归一化函数（Normalizing Function）、使用了旋转位置嵌入（RoPE）、激活函数由ReLU更换为SwiGLU，并且将self-attention改进为使用KV-Cache的Grouped Query，整体Transformer架构与GPT-2 类似。

**Gemma：**

来自Google。

**Mistral：**

【参考】https://www.51cto.com/article/782682.html



开源模型

**LLM**
baichuan（百川2、3）        百川智能（王小川）

qwen（千问1.5）                阿里

InternLLM（书生、浦语）   上海人工智能实验室AILab

ChatGLM3                           智谱AI and 清华大学

OLMo：完全开源

Lit-LLaMA

Dolly

DeepSeek-Coder/DeepSeekMath



**miniLLM:**

TinyLlama：1.1B

LiteLlama

MiniCPM： 2B

ChatLM-mini-Chinese： 0.2B

MindLLM：1.3B、3B

SLM-LiteLlama：0.46B

TinyLLM：https://github.com/jasonacox/TinyLLM    https://github.com/zozoheir/tinyllm

Mini-llm：https://github.com/jiahe7ay/MINI_LLM    https://github.com/dst1213/MINI_LLM



完全开源：

OLMo： allenai

LLM360

一个小模型的训练过程：https://github.com/DLLXW/baby-llama2-chinese

另外一个训练过程：https://github.com/jiahe7ay/MINI_LLM